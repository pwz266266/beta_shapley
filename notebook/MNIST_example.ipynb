{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example MNIST dataset\n",
    "- In this notebook, we introduce how to use `ShapEngine` to compute Beta Shapley value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/wp2280/cuda_lib_temp/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-27 23:34:26.579537: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-27 23:34:26.681043: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/wp2280/cuda_lib_temp/\n",
      "2022-04-27 23:34:26.682008: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 17113836229084143116\n",
      "xla_global_id: -1\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import sys, argparse, random\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4\"\n",
    "os.environ[\"LD_LIBRARY_PATH\"]=\"/home/wp2280/cuda_lib_temp/\"\n",
    "\n",
    "! echo $LD_LIBRARY_PATH\n",
    "\n",
    "sys.path.append('../betashap')\n",
    "\n",
    "from time import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "np.random.seed(2022)\n",
    "import utils, data\n",
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Original Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-27 23:34:28.547317: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# train_labels = train_labels[:1000]\n",
    "# test_labels = test_labels[:1000]\n",
    "#\n",
    "# train_images = train_images[:1000].reshape(-1, 28 * 28) / 255.0\n",
    "# test_images = test_images[:1000].reshape(-1, 28 * 28) / 255.0\n",
    "\n",
    "train_labels = tf.one_hot(train_labels, 10)\n",
    "test_labels = tf.one_hot(test_labels, 10)\n",
    "\n",
    "train_images = tf.expand_dims(train_images, -1)\n",
    "test_images = tf.expand_dims(test_images, -1)\n",
    "train_images = tf.cast(train_images, tf.float32) / 255.0\n",
    "test_images = tf.cast(test_images, tf.float32) / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Beta Shapley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args():\n",
    "    def __init__(self):\n",
    "        self.rate = 0.001\n",
    "        self.epoch = 1\n",
    "        self.seed = 1\n",
    "        self.loss = ''\n",
    "        self.checkpoint = ''\n",
    "        self.path = 'checkpoint'\n",
    "        self.activation = 'relu'\n",
    "        self.save_checkpoint = False\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def faultyloss(probs, labels):\n",
    "    \"\"\"Calculates cross entropy loss.\n",
    "    Args:\n",
    "      probs: Class probabilities predicted by the model. The shape is expected\n",
    "        to be (?, 10).\n",
    "      labels: Truth labels for the classes, as one-hot encoded vectors. The\n",
    "        shape is expected to be the same as `probs`.\n",
    "    Returns:\n",
    "      A scalar loss tensor.\n",
    "    \"\"\"\n",
    "    diff = -labels * tf.math.log(probs)\n",
    "    loss = tf.reduce_mean(diff)\n",
    "    return loss\n",
    "\n",
    "def create_model(loss=tf.losses.CategoricalCrossentropy(from_logits=False), activation='relu', lr=0.001):\n",
    "    model_classifier = tf.keras.models.Sequential([\n",
    "      keras.layers.Conv2D(32, [3, 3], activation=activation, input_shape=(28, 28, 1)),\n",
    "      keras.layers.Conv2D(64, [3, 3], activation=activation),\n",
    "      keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "      keras.layers.Dropout(0.25),\n",
    "      keras.layers.Flatten(),\n",
    "      keras.layers.Dense(128, activation=activation),\n",
    "      keras.layers.Dropout(0.5),\n",
    "      keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model_extractor = tf.keras.Model(inputs=model_classifier.input,\n",
    "                                     outputs=model_classifier.layers[-3].output)\n",
    "    model_extractor.compile(run_eagerly=True)\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "    model_classifier.compile(optimizer=optimizer,\n",
    "                  loss=loss,\n",
    "                  metrics=[tf.metrics.CategoricalAccuracy()],\n",
    "                  run_eagerly=True)\n",
    "\n",
    "    return model_extractor, model_classifier\n",
    "\n",
    "def train_model(model, checkpoint_path, restore='', epochs=5):\n",
    "    checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "    if restore:\n",
    "        model.load_weights(restore)\n",
    "        print('Restored model from {}'.format(restore))\n",
    "\n",
    "    # Create a callback that saves the model's weights\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_path+\"ck-{epoch:02d}\", verbose=0,\n",
    "        save_weights_only=True)\n",
    "    callbacks = []\n",
    "\n",
    "    if args.save_checkpoint:\n",
    "        callbacks.append(cp_callback)\n",
    "    if args.rate!=0:\n",
    "        keras.backend.set_value(model.optimizer.learning_rate, args.rate)\n",
    "\n",
    "    # Train the model with the new callback \n",
    "    model.fit(train_images,\n",
    "          train_labels,\n",
    "          batch_size=128,\n",
    "          epochs=epochs,\n",
    "          validation_data=(test_images, test_labels),\n",
    "          callbacks=callbacks) # Pass callback to training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wp2280/anaconda3/envs/tf2/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py:4526: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469/469 [==============================] - 25s 54ms/step - loss: 0.2377 - categorical_accuracy: 0.9282 - val_loss: 0.0437 - val_categorical_accuracy: 0.9863\n",
      "Processed  0  samples\n",
      "Processed  0  samples\n"
     ]
    }
   ],
   "source": [
    "# Create a basic model instance\n",
    "loss_func = tf.losses.CategoricalCrossentropy(from_logits=False)\n",
    "if args.loss == '' or args.loss == 'entropy':\n",
    "    loss_func = tf.losses.CategoricalCrossentropy(from_logits=False)\n",
    "elif args.loss == 'faulty':\n",
    "    print(\"Using faulty loss\")\n",
    "    loss_func = faultyloss\n",
    "elif args.loss == 'mse':\n",
    "    loss_func = tf.losses.MeanSquaredError()\n",
    "extractor, model = create_model(loss=loss_func, activation=args.activation, lr=args.rate)\n",
    "result = train_model(model, args.path+'/', restore=args.checkpoint, epochs=args.epoch)\n",
    "\n",
    "test_features = []\n",
    "# for i in range(int(test_images.shape[0]/1000)):\n",
    "for i in range(1):\n",
    "    print('Processed ', i*100, ' samples')\n",
    "    test_features.append(extractor.predict(test_images[i*100:i*100+100]))\n",
    "test_features = tf.concat(test_features, 0)\n",
    "\n",
    "train_features = []\n",
    "train_features = []\n",
    "for i in range(int(train_images.shape[0]/1000)):\n",
    "    print('Processed ', i*100, ' samples')\n",
    "    train_features.append(extractor.predict(train_images[i*100:i*100+100]))\n",
    "train_features = tf.concat(train_features, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 128)\n",
      "(20, 128)\n",
      "(20,)\n",
      "(20,)\n",
      "Source is initialized. A unit of sample is one data point\n",
      "Start: Marginal Contribution Calculation!\n",
      "Here\n",
      "Here\n",
      "Here\n",
      "Here\n"
     ]
    }
   ],
   "source": [
    "# Evaluate value\n",
    "from ShapEngine import ShapEngine\n",
    "\n",
    "test_features = test_features[:20]\n",
    "train_features = train_features[:20]\n",
    "test_features_np = test_features.numpy()\n",
    "train_features_np = train_features.numpy()\n",
    "test_labels_np = test_labels.numpy()[0:20]\n",
    "train_labels_np = train_labels.numpy()[0:20]\n",
    "train_labels_np = np.argmax(train_labels_np, axis=1)\n",
    "test_labels_np = np.argmax(test_labels_np, axis=1)\n",
    "\n",
    "model_family='logistic'\n",
    "metric='accuracy'\n",
    "GR_threshold=1.05\n",
    "weights_list=[(1, 16), (1, 4), (1,1), (4,1), (16, 1)]\n",
    "\n",
    "shap_engine=ShapEngine(X=train_features_np, y=train_labels_np, X_val=test_features_np, y_val=test_labels_np, \n",
    "                       problem='classification', model_family=model_family, \n",
    "                       metric=metric, GR_threshold=GR_threshold)\n",
    "%time shap_engine.run(weights_list=weights_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of data values: ['Beta(16,1)', 'Beta(4,1)', 'Beta(1,1)', 'Beta(1,4)', 'Beta(1,16)', 'LOO-Last']\n"
     ]
    }
   ],
   "source": [
    "# A vector of data values is stored in `shap_engine.results` \n",
    "print(f'List of data values: {list(shap_engine.results.keys())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 data values: [0.0288301  0.01684534 0.02828557 0.0250167  0.02825778 0.02424356\n",
      " 0.02590261 0.02547816 0.02338693 0.0285735 ]\n"
     ]
    }
   ],
   "source": [
    "# Beta(4:1) data values for the first 10 data points. \n",
    "first_ten_values=shap_engine.results['Beta(4,1)'][:10]\n",
    "print(f'First 10 data values: {first_ten_values}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of MC_mat : (100, 100)\n",
      "First 12 marginal contributions of the first sample:\n",
      " [ 0.00000000e+00  3.46944695e-18  6.08333333e-02  6.50000000e-02\n",
      " -1.00000000e-02  4.66666667e-02  5.71428571e-02  4.30769231e-02\n",
      "  4.20000000e-02  4.75000000e-02  3.40000000e-02  2.75000000e-02]\n"
     ]
    }
   ],
   "source": [
    "MC_mat=shap_engine.MC_obs_card_mat/(shap_engine.MC_count_obs_card_mat+1e-16)\n",
    "print(f'Shape of MC_mat : {MC_mat.shape}')\n",
    "print(f'First 12 marginal contributions of the first sample:\\n {np.array(MC_mat[0])[:12]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Marginal contributions for clean and noisy samples\n",
    "- Figure 2 in the manuscript shows a smooth curve because it is based on 50 independent runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'noisy_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/wp2280/beta_shapley/notebook/MNIST_example.ipynb Cell 16'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bthor.cs.columbia.edu/home/wp2280/beta_shapley/notebook/MNIST_example.ipynb#ch0000015vscode-remote?line=0'>1</a>\u001b[0m clean_index\u001b[39m=\u001b[39m[i \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(MC_mat\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]) \u001b[39mif\u001b[39;00m i \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m noisy_index]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bthor.cs.columbia.edu/home/wp2280/beta_shapley/notebook/MNIST_example.ipynb#ch0000015vscode-remote?line=1'>2</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(np\u001b[39m.\u001b[39marange(MC_mat\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]), np\u001b[39m.\u001b[39mmean(MC_mat[clean_index], axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m), label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mClean\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bthor.cs.columbia.edu/home/wp2280/beta_shapley/notebook/MNIST_example.ipynb#ch0000015vscode-remote?line=2'>3</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(np\u001b[39m.\u001b[39marange(MC_mat\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]), np\u001b[39m.\u001b[39mmean(MC_mat[noisy_index], axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m), label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mNoisy\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32m/home/wp2280/beta_shapley/notebook/MNIST_example.ipynb Cell 16'\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bthor.cs.columbia.edu/home/wp2280/beta_shapley/notebook/MNIST_example.ipynb#ch0000015vscode-remote?line=0'>1</a>\u001b[0m clean_index\u001b[39m=\u001b[39m[i \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(MC_mat\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]) \u001b[39mif\u001b[39;00m i \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m noisy_index]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bthor.cs.columbia.edu/home/wp2280/beta_shapley/notebook/MNIST_example.ipynb#ch0000015vscode-remote?line=1'>2</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(np\u001b[39m.\u001b[39marange(MC_mat\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]), np\u001b[39m.\u001b[39mmean(MC_mat[clean_index], axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m), label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mClean\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bthor.cs.columbia.edu/home/wp2280/beta_shapley/notebook/MNIST_example.ipynb#ch0000015vscode-remote?line=2'>3</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(np\u001b[39m.\u001b[39marange(MC_mat\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]), np\u001b[39m.\u001b[39mmean(MC_mat[noisy_index], axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m), label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mNoisy\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'noisy_index' is not defined"
     ]
    }
   ],
   "source": [
    "clean_index=[i for i in range(MC_mat.shape[0]) if i not in noisy_index]\n",
    "plt.plot(np.arange(MC_mat.shape[0]), np.mean(MC_mat[clean_index], axis=0), label='Clean')\n",
    "plt.plot(np.arange(MC_mat.shape[0]), np.mean(MC_mat[noisy_index], axis=0), label='Noisy')\n",
    "plt.legend(fontsize=15)\n",
    "plt.xlabel('Cardinality', fontsize=15)\n",
    "plt.ylabel('Marginal Contributions', fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance on downstream ML tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.7 s, sys: 286 ms, total: 21 s\n",
      "Wall time: 3.58 s\n",
      "Available ML tasks: ['noisy', 'subsampling', 'point_removal', 'point_addition']\n"
     ]
    }
   ],
   "source": [
    "%time result_dict=utils.summary_experiments(shap_engine, noisy_index, X_test, y_test)\n",
    "print(f'Available ML tasks: {list(result_dict.keys())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For each method, the following dictionary shows recall, precision, and F1-score.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Beta(16,1)': [0.35, 0.3888888888888889, 0.3684210526315789],\n",
       " 'Beta(4,1)': [0.45, 0.45, 0.44999999999999996],\n",
       " 'Beta(1,1)': [0.5, 0.35714285714285715, 0.4166666666666667],\n",
       " 'Beta(1,4)': [0.35, 0.14285714285714285, 0.20289855072463767],\n",
       " 'Beta(1,16)': [0.3, 0.11764705882352941, 0.16901408450704225],\n",
       " 'LOO-Last': [0.15, 0.0625, 0.08823529411764705]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For instance, a noisy label detection task result is stored in result_dict['noisy'] \n",
    "print('For each method, the following dictionary shows recall, precision, and F1-score.')\n",
    "result_dict['noisy'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
